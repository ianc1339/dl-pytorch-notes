{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95292ca2-bc58-4840-b4d2-04176061196e",
   "metadata": {},
   "source": [
    "# Chapter 1 - Introducing deep learning and the Pytorch Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325da899-ee41-46dd-a5fd-2e5ffb37347f",
   "metadata": {},
   "source": [
    "Deep learning uses large amounts of data to approximate complex, non-linear functions.\n",
    "\n",
    "Machine learning relies heavily on feature engineering (transforming input data). On the other hand, deep learning automatically finds representations from raw data to successfully perform a task.\n",
    "\n",
    "\"Requirements for successful deep learning\":\n",
    "- A way to take in data\n",
    "- A good definition for the deep learning machine\n",
    "- Training method to obtain useful representation from input data to produce desired output\n",
    "\n",
    "PyTorch's core data structure is Tensor (a multidimensional array). It shares many similarities with NumPy arrays.\n",
    "\n",
    "PyTorch is simplistic and provides accelerated computation with GPUs. Since PyTorch is equipped with a high-performance C++ runtime, PyTorch models can also be used in production, without fully relying on Python. In fact, most of PyTorch is written in C++ and CUDA.\n",
    "\n",
    "PyTorch tensors and operators on tensors can be used on either the CPU or the GPU. Moving computations from CPU to GPU is relatively simple in PyTorch.\n",
    "\n",
    "These tensors can also keep track of operations performed on them and analytically compute derivatives of an ouput respect to its inputs. This can be used for numerical optimizations (Pytorch's autograd engine).\n",
    "\n",
    "The core PyTorch modules for building neural networks are located in `torch.nn` (provides common network layers and other components).\n",
    "\n",
    "When training a model, the training data has to be converted to tensors so that Pytorch can handle them. The `Dataset` class in `torch.utils.data` bridges the gap between an arbitrary training data and tensors. In addition, the `DataLoader` class allows for parallel data loading by assembling data into batches.\n",
    "\n",
    "At each step of the training loop, a loss function is used to evalulate the difference between the model's output and the desired output (the loss functions are also provided in `torch.nn`). Afterwards, the model is modified to resemble the desired behaviour. This is where PyTorch's autograd engine and optimizer comes in (where the optimizer is provided in `torch.optim`).\n",
    "\n",
    "More elaborate hardware like multiple GPUs can be used for training larger models (where `torch.nn.parallel.DistributedDataParallel` and the `torch.distributed` submodule can be used).\n",
    "\n",
    "After the training loop has completed, the trained model is often deployed to become useful (e.g. cloud engine or integration with larger application).\n",
    "\n",
    "TorchScript can also be used to compile models ahead of time such that the model can be run independently from Python (e.g. from C++ programs or on mobile devices)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
